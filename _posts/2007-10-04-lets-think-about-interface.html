---
layout: post
title: Let’s think about Interface…
date: 2007-10-04 17:45:45.000000000 +01:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Technology
tags: []
meta:
  dsq_thread_id: '597010186'
  blr_date: '2011-10-14'
  _jetpack_related_posts_cache: a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1451786545;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:215;}i:1;a:1:{s:2:"id";i:276;}i:2;a:1:{s:2:"id";i:204;}}}}
author:
  login: andy
  email: andy@zrmt.com
  display_name: andylockran
  first_name: Andy
  last_name: Loughran
---
<p>User Interface design has really started to grab my interest over the last few years.  There are so many different electronic tools that we use, some with massively simple interfaces, and others with very complex ones.  We use both everyday.  I would say any interface which uses the Xerox Windowing system (Windows, OSX, Gnome, KDE, to name a few) is a massively complex interface.  This may or may not be a bad thing, but I think it warrants futher investigation.</p>
<p>One of the concepts I came across whilst studying some aspects of interface design at Uni were the way in which cross-discipline research was retrospectively applied to interface design.  The biggest factor was Brown's (1952) 7 (+/-2)  theory on memory.  He argued that - in short term memory - we could only remember around 7 (plus or minus two) items at a time, before the next one we learnt pushed the one before it out (so to speak).  Now the resultant 'policy' by some UI designers was to specificy that all menus should have no greater than seven items.  Looking up at my firefox toolbar, there is 'File', 'Edit', 'View', 'History', 'Bookmarks', 'Tools', &amp; 'Help'.  - seven items.</p>
<p>Whilst this may be tidy and neat, and appears to fulfil the requirements of academic study, there's also the case for saying that I don't need to keep those items in my short term memory as they are always on the screen in front of me.  Perhaps a better application would be that a menu go no further than  levels - but then again, all those levels are now available on the screen so do not need to be kept in memory.  It's an interesting concept, and currently one I have no answer to.</p>
<p>I ran a small experiment in my third year which compared visual stimuli information recognition from a ten minute lecture to an audiovisual representation of the same lecture (where the information presented on the screen was read out simultaneously).  Surprisingly I found there was no dramatic difference between the two conditions.  However, after a recent conversation with the wife of a prospective colleague, I discovered that the same experiment would yield massive differences for the disphraxic/dyslexic students that she worked with.  Using this as an example - how can I justify designing an interface in such a way that allows some people the ability to utilise a computer to maximum efficiency (both in terms of time and computational resources) yet cripple the attempts of another group - who, due to government policy and their own disadvantages, use the interface much more as a tool for advancing their own efficiency?</p>
<p>There are tools (green dragon if I recall correctly) that can be used as aids by dyslexic computer users to help them with their typing and spelling.   I recall an incident in my second year when a fresher called at our flats, as she'd heard there was a 'computer geek' living there.  I duly obliged and went to help the girl with her problem.   In order to use her software, she'd had to cut and paste everything from MS word, into her software, and back again.  Hardly an efficient user interface for someone who was using the software to increase their own efficiency.  As it turns out, the data she had been typing I was unable to recover.</p>
<p>The only way I see of changing behaviour such as the one outlined above is by a massive integration of a number of systems.  There's only so much a programmer can do if their software is layered on top of a closed operating system.  What is needed is the ability to interact with the core of the OS so that such events don't happen.   Whilst there are some very skilled programmers that would suggest that it's quite simple to create software that will run on top that allows the behaviour necessary to prevent accidental deletion - there is not a way to prevent purposeful deletion, unless ingrained in the OS.  It's an interesting avenue of thought.</p>
<p>The base of these convoluted and disjointed arguments is that there needs to be a rethink of interface.  It's not just about windowing managers being made to look pretty.  Compiz/Fusion does that already.  It's about how all the pieces of the jigsaw fit together.  It's a massive project, and one that can either be done by stripping down what we currently have and building it up again, or doing the 'Debian approach' and starting with the minimal system, and building something on top of that.  Unfortunately, I don't have the skills necessary to begin or even contemplate such a project - but whilst we have many companies that will suggest inteface design changes to get the maximum number of click-throughs on a website, we don't yet have enough people suggesting interface design changes for the averag PC.  Apple have made some improvements to what a majority of people were provided with in XP, but the problem is deeper.  Is windowing even the solution?</p>
